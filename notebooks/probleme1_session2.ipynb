{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Problème - Session n°2 : une variable cachée"
      ],
      "metadata": {
        "id": "GYE9s4-HA1o_"
      },
      "id": "GYE9s4-HA1o_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce problème, on travaille sur un jeu de données comportant 50.000 entrées $x_i$ et des cibles $y_i$. Les entrées sont des vecteurs de taille 10 (au format torch), les cibles sont des scalaires construits à partir de cinq fonctions différentes ($f_0$, ..., $f_4$) : \\\n",
        "\n",
        "$$ \\forall i, \\exists k\\in [\\![0 \\;;4]\\!]  \\:\\: \\text{tel que} \\: f_k(x_i) = y_i $$\n",
        "\n",
        "Ces fonctions sont inconnues, ainsi que l'indice $k$. Par contre, on sait que le groupe des 1000 premières cibles ont été construites à partir du même indice  $k$, de même pour les mille  suivantes, et ainsi de suite.\n",
        "\n",
        "Le but est de parvenir à rassembler les groupes de cibles qui ont été générées avec le même indice $k$ (avec la même fonction)."
      ],
      "metadata": {
        "id": "pknhArHLLPP-"
      },
      "id": "pknhArHLLPP-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example d'échantillonnage du dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "! git clone https://github.com/messagerjulien/exam_2025_session2.git\n",
        "! cp exam_2025_session2/utils/utils.py .\n",
        "from utils import Problem1Dataset\n",
        "\n",
        "dataset = Problem1Dataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    x_batch, y_batch, k_batch, idx_batch = batch\n",
        "    print(\"Batch input shape:\", x_batch.shape)\n",
        "    print(\"Batch target shape:\", y_batch.shape)\n",
        "    print(\"Batch k shape:\", k_batch.shape) # indice k (pas utilisable à l'entraînement)\n",
        "    print(\"Batch indices shape:\", idx_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "zqFIZKgTPK14",
        "outputId": "b10fdc52-962a-4561-8f31-0c06540fc86b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zqFIZKgTPK14",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exam_2025_session2' already exists and is not an empty directory.\n",
            "Batch input shape: torch.Size([32, 10])\n",
            "Batch target shape: torch.Size([32, 1])\n",
            "Batch k shape: torch.Size([32])\n",
            "Batch indices shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consignes :**\n",
        "- Entraîner l'architecture proposée dans la cellule suivante.\n",
        "- Montrer que les vecteurs 2D de self.theta permettent de répondre\n",
        "  au problème posé.\n",
        "- Décrire le rôle de self.theta, du vector noise \\\n",
        " et ainsi que la raison de la division par 1000 (**indices // 1000** dans le code)."
      ],
      "metadata": {
        "id": "-q1zuebPPvob"
      },
      "id": "-q1zuebPPvob"
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
        "        super(DeepMLP, self).__init__()\n",
        "        self.theta = nn.Parameter(torch.randn(50, 2))\n",
        "        self.fc1 = nn.Linear(input_dim + 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, indices):\n",
        "        theta_batch = self.theta[indices // 1000, :]\n",
        "        noise = torch.normal(mean=torch.zeros_like(theta_batch),\n",
        "                             std=torch.ones_like(theta_batch))\n",
        "        x = torch.cat([x, theta_batch + noise], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x, theta_batch"
      ],
      "metadata": {
        "id": "idbO80HfPdiQ"
      },
      "id": "idbO80HfPdiQ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2000\n",
        "batch_size = 50 # A régler\n",
        "loss_fn = nn.CrossEntropyLoss() # Standard pour ce type de tâche\n",
        "\n",
        "model = DeepMLP(10, 5)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=10**(-3))\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "    # random traversal of the dataset\n",
        "    for x, y, k, indice in dataloader:\n",
        "        # zeroing gradients\n",
        "        optimizer.zero_grad()\n",
        "        # calculation of (p0, p1)\n",
        "\n",
        "        output, theta = model(x, indice) #J'ai voulu passer sur GPU mais j'avais une erreur donc j'ai enlevé (\".cuda()\")\n",
        "        # calculation of the error\n",
        "\n",
        "        l = loss_fn(output, k)\n",
        "        # calculation of gradients\n",
        "        l.backward()\n",
        "        # weight update\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "UVmpuK5mnXY5",
        "outputId": "e14bde10-4750-43dd-fe56-4a029f899968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UVmpuK5mnXY5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "Epoch 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "self.theta représente un tenseur de paramètres qui sont disponibles uniquement pour cette classe. Il est de taille 50x2 et on rajoute theta en plus d'un bruit aux données d'entrées.\n",
        "\n",
        "Le noise est un bruit centré réduit de la même taille que le tenseur theta.\n",
        "\n",
        "On divise par mille parce qu'on sait d'après l'énoncé que les entrées sont créés par la même fonction, par groupe de 1000. Ainsi on utilise les mêmes paramètres de theta pour un même groupe de vecteurs."
      ],
      "metadata": {
        "id": "pToNfM6BSuYS"
      },
      "id": "pToNfM6BSuYS"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}